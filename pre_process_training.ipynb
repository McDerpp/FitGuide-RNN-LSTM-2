{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjXOMwl5zFmJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard,ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "# import mediapipe as mp\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from tensorflow_model_optimization.quantization.keras import quantize_model\n",
        "from collections import Counter\n",
        "import random as rand\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuLyvOQ-v3Ud"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def txt_pre_process(txt_file,label,simplify=False,simplify_level=14 ):\n",
        "    label_array = []\n",
        "    temp_feature_data = []\n",
        "    temp_sequence_data = []\n",
        "    batch_data = []\n",
        "\n",
        "    with open(str(txt_file), 'r') as file:\n",
        "\n",
        "        for line in file:\n",
        "            values = line.strip().split('|')\n",
        "\n",
        "            temp_feature_data = []\n",
        "\n",
        "            for value in values:\n",
        "                float_value = str(value)\n",
        "\n",
        "                #FIRST PART OF THE SEQUENCE\n",
        "                if float_value == 'START':\n",
        "                    temp_sequence_data=[]\n",
        "\n",
        "                elif float_value == 'END':\n",
        "                    batch_data.append(temp_sequence_data)\n",
        "                    label_array.append(label)\n",
        "\n",
        "\n",
        "                elif float_value != '' and float_value != 'START':\n",
        "                    if simplify:\n",
        "                        float_value = round(float(value),simplify_level)\n",
        "                    else:\n",
        "                        float_value = float(value)\n",
        "                    temp_feature_data.append(float_value)\n",
        "\n",
        "            if temp_feature_data!=[]:\n",
        "                temp_sequence_data.append(temp_feature_data)\n",
        "\n",
        "    label_array = np.array(label_array)\n",
        "    return [batch_data,label_array]\n",
        "\n",
        "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
        "# padding can be improved probably...by using sequence\n",
        "# minor issue:\n",
        "# > is whether sequences had exceeded the intended number of sequences but is still right (it was performed right but slower(by an acceptable margin)) - not resolved\n",
        "#    = temporary fix was just to truncate everything if it had exceeded the intended number of sequence for the sake of running it for now\n",
        "#    = a reliable solution in theory could be that to randomly truncate in between the first and end sequence, in this way relevant data can be captured\n",
        "def padding(pre_processed_input,optional_maxLength=0):\n",
        "    padded_sequences = []\n",
        "    if optional_maxLength != 0:\n",
        "        max_length = optional_maxLength\n",
        "    else:\n",
        "        max_length = max(len(sequence) for sequence in pre_processed_input)\n",
        "\n",
        "    for sequence in pre_processed_input:\n",
        "        padding_length = max_length - len(sequence)\n",
        "        if padding_length >= 0:\n",
        "            padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
        "\n",
        "        else:\n",
        "            padded_sequence = sequence[:max_length]\n",
        "        padded_sequences.append(padded_sequence)\n",
        "    padded_sequences = np.array(padded_sequences)\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
        "\n",
        "# this is to merge correct executions and wrong executions and randomize their input and label\n",
        "# positions of input and its corresponding label are the same\n",
        "# introducing noise/wrong input makes the model more robust\n",
        "def concatenate_randomize_batches(base_input,base_label,concat_input,concat_label):\n",
        "    combined_inputs = np.concatenate((base_input,concat_input), axis = 0)\n",
        "    combined_label = np.concatenate((base_label,concat_label), axis = 0)\n",
        "    indices = np.random.permutation(len(combined_inputs))\n",
        "    randomized_inputs = combined_inputs[indices]\n",
        "    randomized_label = combined_label[indices]\n",
        "    return [randomized_inputs,randomized_label]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tally_sequence(sequence_array):\n",
        "    tally_number = []\n",
        "    tally_ctr = []\n",
        "\n",
        "    for x in sequence_array:\n",
        "        temp = len(x)\n",
        "        if temp not in tally_number:\n",
        "            tally_number.append(temp)\n",
        "            tally_ctr.append(1)\n",
        "        else:\n",
        "            for y in range(len(tally_number)) :\n",
        "                if temp == tally_number[y]:\n",
        "                    tally_ctr[y] = tally_ctr[y] + 1\n",
        "\n",
        "    tally_max = 0\n",
        "    tally_number_arranged = []\n",
        "    tally_ctr_arranged = []\n",
        "\n",
        "    for x in range(len(tally_number)):\n",
        "        # print(len(tally_ctr))\n",
        "        tally_max = max(tally_ctr)\n",
        "        for y in range(len(tally_number)):\n",
        "            if tally_ctr[y] == tally_max:\n",
        "                tally_number_arranged.append(tally_number[y])\n",
        "                tally_ctr_arranged.append(tally_ctr[y])\n",
        "                tally_ctr.pop(y)\n",
        "                tally_number.pop(y)\n",
        "                break\n",
        "\n",
        "    total_ctr = 0\n",
        "    for x in tally_ctr:\n",
        "        total_ctr = total_ctr + x\n",
        "\n",
        "\n",
        "    for x in range(len(tally_number_arranged)):\n",
        "        print(tally_number_arranged[x],'-->',tally_ctr_arranged[x])\n",
        "\n",
        "\n",
        "# outlier detection and removal (currently being used)\n",
        "def common_length_sequence(sequences_array,threshold = 2):\n",
        "    temp = []\n",
        "\n",
        "    data = [len(seq) for seq in sequences_array]\n",
        "    data_frequency = Counter(data)\n",
        "    most_common_data = data_frequency.most_common()\n",
        "    outlier_frequencies = [value for value, freq in data_frequency.items() if freq < threshold]\n",
        "    most_common_values = [value for value, freq in most_common_data if freq >= threshold]\n",
        "\n",
        "    print(\"Most Common Data Points:\", most_common_values)\n",
        "    print(\"Outlier Frequencies:\", outlier_frequencies)\n",
        "\n",
        "    for x in sequences_array:\n",
        "        if len(x) in most_common_values:\n",
        "            temp.append(x)\n",
        "    print('-------------------applied frequency outlier detection-------------------')\n",
        "    print(\"original num -> \", len(sequences_array))\n",
        "    print(\"current num -> \", len(temp))\n",
        "    print(\"removed num -> \", len(sequences_array) - len(temp))\n",
        "    return temp\n",
        "\n",
        "# outlier detection and removal (currently being used)\n",
        "def apply_z_score(sequences_array,z_score_threshold = 1):\n",
        "    data_points = []\n",
        "    included_datapoints = []\n",
        "    updated_sequences =[]\n",
        "\n",
        "    for x in sequences_array:\n",
        "        temp = len(x)\n",
        "        if temp not in data_points:\n",
        "            data_points.append(temp)\n",
        "\n",
        "    data = np.array(data_points)\n",
        "    mean_value = np.mean(data)\n",
        "    standard_deviation = np.std(data)\n",
        "    z_scores = (data - mean_value) / standard_deviation\n",
        "    for x in range(len(z_scores)):\n",
        "        if np.abs(z_scores[x]) <= z_score_threshold:\n",
        "            included_datapoints.append(data[x])\n",
        "\n",
        "\n",
        "    for x in sequences_array:\n",
        "        if len(x) in included_datapoints:\n",
        "            updated_sequences.append(x)\n",
        "    print('-------------------applied z-score outlier detection-------------------')\n",
        "    print(\"datapoints included -> \", included_datapoints)\n",
        "    print(\"original num -> \", len(sequences_array))\n",
        "    print(\"current num -> \", len(updated_sequences))\n",
        "    print(\"removed num -> \", len(sequences_array) - len(updated_sequences))\n",
        "\n",
        "    return updated_sequences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def paddingV2(sequences_array_input,optional_maxlength = 0):\n",
        "    sequences_array = copy.deepcopy(sequences_array_input)\n",
        "\n",
        "\n",
        "    output = []\n",
        "    max_length = 0\n",
        "    if optional_maxlength == 0:\n",
        "        max_length = max(len(sequence) for sequence in sequences_array)\n",
        "        expanded_max_length = int(max_length+ ((max_length) * .10))\n",
        "    else:\n",
        "        expanded_max_length = optional_maxlength\n",
        "\n",
        "    # sequence = np.array(sequences_array)\n",
        "\n",
        "    print(expanded_max_length)\n",
        "\n",
        "\n",
        "    padding_length_before = 0\n",
        "    padding_length_after = 0\n",
        "\n",
        "    for seq in sequences_array:\n",
        "        # print(seq)\n",
        "        for x in range(expanded_max_length-len(seq)+1):\n",
        "            padding_length_before = x\n",
        "            padding_length_after = expanded_max_length - len(seq) - x\n",
        "            padded_sequence = np.pad(seq, ((padding_length_before, padding_length_after),(0,0)), mode='constant')\n",
        "            output.append(padded_sequence)\n",
        "\n",
        "            # print(padded_sequence)\n",
        "    print('------------------------applied paddingV2------------------------')\n",
        "    print('max_length -> ', max_length)\n",
        "    print('expanded_max_length -> ', expanded_max_length)\n",
        "    print('original num set of sequences -> ', len(sequences_array))\n",
        "    print('final num set of sequences -> ', len(output))\n",
        "\n",
        "    output = np.array(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_tf_to_tflite(tf_model,input_shape,test_dataset,name,id_number,validation_loss,validation_accuracy):\n",
        "  model = tf.keras.models.load_model(tf_model)\n",
        "\n",
        "  run_model = tf.function(lambda x: model(x))\n",
        "  # This is important, let's fix the input size.\n",
        "  BATCH_SIZE = input_shape[0]\n",
        "  STEPS = input_shape[1]\n",
        "  INPUT_SIZE = input_shape[2]\n",
        "  concrete_func = run_model.get_concrete_function(\n",
        "      tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
        "\n",
        "  # model directory.\n",
        "  MODEL_DIR = \"keras_lstm\"\n",
        "  model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
        "  tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "  # Run the model with TensorFlow to get expected results.\n",
        "  TEST_CASES = 10\n",
        "\n",
        "  # Run the model with TensorFlow Lite\n",
        "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  for i in range(TEST_CASES):\n",
        "    expected = model.predict(test_dataset[i:i+1])\n",
        "    interpreter.set_tensor(input_details[0][\"index\"], test_dataset[i:i+1, :, :])\n",
        "    interpreter.invoke()\n",
        "    result = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "\n",
        "    # Assert if the result of TFLite model is consistent with the TF model.\n",
        "    np.testing.assert_almost_equal(expected, result, decimal=5)\n",
        "    print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
        "\n",
        "    interpreter.reset_all_variables()\n",
        "\n",
        "\n",
        "\n",
        "  temp = 'converted_model_'\n",
        "\n",
        "  temp3 = temp + str(name) + id_number + \"(loss_\"+ str(round(validation_loss,3)) +\")\" + \"(acc_\"+  str(round(validation_accuracy,3 )) + \")\" + '.tflite'\n",
        "  # Save the TFLite model to a file\n",
        "  with open(temp3, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "  # with open(\"converted_model.tflite\", \"wb\") as f:\n",
        "  #     f.write(tflite_model)\n",
        "\n",
        "\n",
        "# this data augmentation replaces padded index with random inputs\n",
        "def populate_0_input(correct_data_input,noise_data_input):\n",
        "    correct_data = copy.deepcopy(correct_data_input)\n",
        "    noise_data = copy.deepcopy(noise_data_input)\n",
        "\n",
        "    print(len(correct_data))\n",
        "    index = 10\n",
        "    temp = []\n",
        "    temp_compilation = []\n",
        "    ctr = 0\n",
        "    rand_modifier =0\n",
        "\n",
        "    for set_sequence in tqdm(correct_data, desc=\"populate_0_input\", leave=True):\n",
        "        rand_modifier = rand.randint(0,len(noise_data))\n",
        "\n",
        "        for x in range(len(set_sequence)):\n",
        "            ctr = ctr + 1\n",
        "            if set_sequence[x][0] == 0:\n",
        "                temp.append(noise_data[rand_modifier-1][rand.randint(0,len(noise_data[rand_modifier-1])-1)])\n",
        "\n",
        "            else:\n",
        "                temp.append(set_sequence[x])\n",
        "\n",
        "        temp_compilation.append(temp)\n",
        "        temp =[]\n",
        "\n",
        "\n",
        "    return temp_compilation\n",
        "\n",
        "\n",
        "\n",
        "class CustomEarlyStopping(Callback):\n",
        "  def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10):\n",
        "      super(CustomEarlyStopping, self).__init__()\n",
        "      self.accuracy_threshold = accuracy_threshold\n",
        "      self.loss_threshold = loss_threshold\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      if logs is None:\n",
        "          logs = {}\n",
        "\n",
        "      if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
        "          return\n",
        "\n",
        "      if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
        "          self.model.stop_training = True\n",
        "          print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
        "                f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
        "\n",
        "\n",
        "# this data augmentation augments data to simulate if the sequence were follow,whether there is unecessary movements in between.\n",
        "# this would also reinforce the model to detect certain action that were classified as correct instead of incorrect\n",
        "# this augmentation is augmenting each sequence of a list of sequence\n",
        "# 1 set of sequence(example-> 1 push up):\n",
        "# [seq1,seq2,seq3,seq4]\n",
        "# [seq1,seq2,NOISE_SEQ34,seq4]\n",
        "def data_aug_sensitivity(sequence_array_list_input,noise_sequence_list_input,num_data_aug = 3,num_aug_in_1_seq = 3,noise_seq_len = 2):\n",
        "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
        "  noise_sequence_list = copy.deepcopy(noise_sequence_list_input)\n",
        "\n",
        "  compile = []\n",
        "  temp_seq = []\n",
        "  temp_storage = []\n",
        "  temp_rand = []\n",
        "  num = 0\n",
        "  ctr1111 = 0\n",
        "  temp_rand2 = 0\n",
        "  temp_rand3 = 0\n",
        "\n",
        "# per sequences\n",
        "  for sequence in tqdm(sequence_array_list, desc=\"data_aug_seq_sensitivity\", leave=True):\n",
        "    # loops for the number of data augmentation per sequence\n",
        "    for ctr in range(num_data_aug):\n",
        "      # loops for the amount of number of augmentation in the sequence(loops to get random index)\n",
        "      while len(temp_rand)!=num_aug_in_1_seq:\n",
        "        num = rand.randint(0,len(sequence)-1)\n",
        "        if num in temp_rand:\n",
        "          continue\n",
        "        else:\n",
        "          temp_rand.append(num)\n",
        "\n",
        "      #actual augmentation of the sequence\n",
        "      temp_seq = sequence.copy()\n",
        "      # store in a temp variable and to be edited\n",
        "\n",
        "      # number of augmentation to be done in a sequence\n",
        "      for ctr1 in range(len(temp_rand)):\n",
        "\n",
        "        # number of sequence to be expanded(index + number of noise_seq_len)\n",
        "        for ctr2 in range(noise_seq_len):\n",
        "          temp_rand2 = rand.randint(0,len(noise_sequence_list)-1)\n",
        "          temp_rand3 = rand.randint(0,len(noise_sequence_list[0])-1)\n",
        "\n",
        "          if (temp_rand[ctr1] + ctr2) < len(temp_seq):\n",
        "            temp_seq[temp_rand[ctr1] + ctr2] = noise_sequence_list[temp_rand2][temp_rand3]\n",
        "\n",
        "\n",
        "          else:\n",
        "            continue\n",
        "\n",
        "\n",
        "      compile.append(temp_seq)\n",
        "      temp_seq = []\n",
        "      temp_rand = []\n",
        "\n",
        "\n",
        "  return compile\n",
        "\n",
        "\n",
        "def data_aug_seq_sensitivity(sequence_array_list_input,num_to_aug=2,num_coor_edit=3,num_sequence_edit=2):\n",
        "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
        "\n",
        "  compile = []\n",
        "  temp = []\n",
        "  rand_coor = []\n",
        "\n",
        "\n",
        "  for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
        "    for sequence in sequence_array_list:\n",
        "      for ctr3 in range(num_sequence_edit):\n",
        "        what_sequence = rand.randint(0,len(sequence)-1)\n",
        "        for ctr2 in range(num_coor_edit):\n",
        "          what_coor = rand.randint(0,len(sequence[0])-1)\n",
        "          rand_coor = rand.randint(0,9999999999)\n",
        "          rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
        "          sequence[what_sequence][what_coor]=rand_coor\n",
        "      compile.append(sequence)\n",
        "  return compile\n",
        "\n",
        "\n",
        "def data_aug_coor_sensitivity(sequence_array_list_input,num_coor_edit=45,num_sequence_edit=8):\n",
        "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
        "\n",
        "  compile = []\n",
        "  temp = []\n",
        "  rand_coor = []\n",
        "  temp_seq = []\n",
        "\n",
        "\n",
        "\n",
        "  # for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
        "  for sequence in sequence_array_list:\n",
        "    temp_seq = sequence.copy()\n",
        "    for ctr3 in range(num_sequence_edit):\n",
        "      what_sequence = rand.randint(0,len(sequence)-1)\n",
        "      num_coor_edit = rand.randint(int(num_coor_edit*.65),num_coor_edit)\n",
        "      for ctr2 in range(num_coor_edit):\n",
        "        what_coor = rand.randint(0,len(sequence[0])-1)\n",
        "        # rand_coor = rand.randint(0,9999999999)\n",
        "        rand_coor = rand.randint(0,999)\n",
        "        rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
        "        temp_seq[what_sequence][what_coor]=rand_coor\n",
        "    compile.append(sequence)\n",
        "  return compile\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [[[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13]],[[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13]],[[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13]],]\n",
        "temp_noise = [[[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0]],[[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0]],[[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0]],]\n",
        "\n",
        "paddingV2(temp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Ns7Pjmuram",
        "outputId": "f2a53f2f-2979-4078-afbd-9286aaeed3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "------------------------applied paddingV2------------------------\n",
            "max_length ->  5\n",
            "expanded_max_length ->  5\n",
            "original num set of sequences ->  3\n",
            "final num set of sequences ->  6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
              "\n",
              "       [[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]],\n",
              "\n",
              "       [[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
              "\n",
              "       [[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
              "\n",
              "       [[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]],\n",
              "\n",
              "       [[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [[[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13]],[[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13]],[[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13],[1,2,3,4,5,6,7,8,9,10,11,12,13]],]\n",
        "temp_noise = [[[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0]],[[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0]],[[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0]],]\n",
        "\n",
        "\n",
        "aug = data_aug_coor_sensitivity(temp,5,5)\n",
        "\n",
        "for x in aug:\n",
        "  print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlQjdtz6qJw3",
        "outputId": "cdabed0a-51da-4a3f-b26f-febc1d3cb113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 0.8690533959, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 0.448573637, 6, 0.5771971458, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0.6471567608, 2, 3, 4, 5, 6, 7, 0.6299563932, 0.8901594111, 0.1089798326, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUsWnjMYddTH",
        "outputId": "0cf54f2a-074b-4018-f1e6-299c58816034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "data_aug_seq_sensitivity: 100%|██████████| 3/3 [00:00<00:00, 4563.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "original ->  [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
            "======================\n",
            "aug ->  [[1, 2, 3, 4, 5], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]] -------------> 0\n",
            "aug ->  [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 2, 3, 4, 5]] -----------> 1\n",
            "\n",
            "original ->  [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
            "======================\n",
            "aug ->  [[1, 2, 3, 4, 5], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [0, 0, 0, 0, 0]] -------------> 2\n",
            "aug ->  [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]] -----------> 3\n",
            "\n",
            "original ->  [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
            "======================\n",
            "aug ->  [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]] -------------> 4\n",
            "aug ->  [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]] -----------> 5\n",
            "qweqwe\n",
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "temp =[[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]],[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]],[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]]]\n",
        "temp_noise =[[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]]\n",
        "print(len(temp))\n",
        "aug = data_aug_sensitivity(temp,temp_noise,2,3,2)\n",
        "\n",
        "# aug = data_aug_coor_sensitivity(temp)\n",
        "# def data_aug_sensitivity(sequence_array_list,noise_sequence_list,num_data_aug = 3,num_aug_in_1_seq = 3,noise_seq_len = 2):\n",
        "for x in range(len(temp)):\n",
        "  print('')\n",
        "  print('original -> ',temp[x])\n",
        "  print('======================')\n",
        "  print('aug -> ',aug[x*2],'------------->',x*2)\n",
        "  print('aug -> ',aug[x*2+1],'----------->',x*2+1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('qweqwe')\n",
        "print(len(aug))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3NYoLxsrZlq",
        "outputId": "b8db243b-798d-4c4e-d093-5fc5c8a694a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test\n",
            "aug-----------\n",
            "[[1, 2, 3, 4], [1, 2, 3, 4], [0.1423094284, 2, 0.7686239527, 0.4367660063]]\n",
            "[[1, 0.5017796175, 3, 0.1073473612], [1, 2, 3, 4], [0.1423094284, 2, 0.7686239527, 0.4367660063]]\n",
            "[[1, 0.9731058537, 0.2545396349, 4], [1, 2, 3, 4], [1, 2, 3, 4]]\n",
            "[[1, 0.9731058537, 0.2545396349, 4], [1, 2, 3, 4], [0.4022334797, 0.9948998469, 3, 0.6104141083]]\n",
            "[[1, 2, 3, 4], [1, 0.8313612199, 0.9256859246, 0.640608401], [1, 2, 3, 4]]\n",
            "[[0.1520550775, 2, 3, 0.7233823363], [1, 0.8313612199, 0.9256859246, 0.640608401], [1, 2, 3, 4]]\n",
            "aug-----------\n"
          ]
        }
      ],
      "source": [
        "num_to_aug=2\n",
        "num_coor_edit=3\n",
        "num_sequence_edit=2\n",
        "print('test')\n",
        "temp = [[[1,2,3,4],[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4],[1,2,3,4]]]\n",
        "compile = []\n",
        "\n",
        "rand_coor = []\n",
        "for ctr in range(num_to_aug):\n",
        "  print('aug-----------')\n",
        "  for sequence in temp:\n",
        "    for ctr3 in range(num_sequence_edit):\n",
        "      what_sequence = rand.randint(0,len(sequence)-1)\n",
        "      for ctr2 in range(num_coor_edit):\n",
        "        what_coor = rand.randint(0,len(sequence[0])-1)\n",
        "        rand_coor = rand.randint(0,9999999999)\n",
        "        rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
        "\n",
        "        sequence[what_sequence][what_coor]=rand_coor\n",
        "      print(sequence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOu1vPu1JOPV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 0 1  -> 0\n",
        "# 2 3  -> 1\n",
        "# 4 5  -> 2\n",
        "# 6 7  -> 3\n",
        "# 8 9  -> 4\n",
        "# 10 11  -> 5\n",
        "# 12 13  -> 6\n",
        "# 14 15  -> 7\n",
        "# 16 17  -> 8\n",
        "# 18 19  -> 9\n",
        "# 20 21  -> 10\n",
        "# 22 23  -> 11\n",
        "# 24 25  -> 12\n",
        "# 26 27  -> 13\n",
        "# 28 29  -> 14\n",
        "# 30 31  -> 15\n",
        "# 32 33  -> 16\n",
        "# 34 35  -> 17\n",
        "# 36 37  -> 18\n",
        "# 38 39  -> 19\n",
        "# 40 41  -> 20\n",
        "# 42 43  -> 21\n",
        "# 44 45  -> 22\n",
        "# 46 47  -> 23\n",
        "# 48 49  -> 24\n",
        "# 50 51  -> 25\n",
        "# 52 53  -> 26\n",
        "# 54 55  -> 27\n",
        "# 56 57  -> 28\n",
        "# 58 59  -> 29\n",
        "# 60 61  -> 30\n",
        "# 62 63  -> 31\n",
        "# 64 65  -> 32\n",
        "\n",
        "# # ----------------------------------------------------------------------------\n",
        "#   left_upper_arm_sequence = []\n",
        "#   left_lower_arm_sequence = []\n",
        "#   left_hand_sequence = []\n",
        "\n",
        "#   right_upper_arm_sequence = []\n",
        "#   right_lower_arm_sequence = []\n",
        "#   right_hand_sequence = []\n",
        "\n",
        "#   left_upper_leg_sequence = []\n",
        "#   left_lower_leg_sequence = []\n",
        "#   left_feet_sequence = []\n",
        "\n",
        "#   right_upper_leg_sequence = []\n",
        "#   right_lower_leg_sequence = []\n",
        "#   right_feet_sequence = []\n",
        "\n",
        "#   body_sequence = []\n",
        "#   head_sequence = []\n",
        "\n",
        "#   # ------------------------------------------------------------------------------\n",
        "#   left_upper_arm_sequence_noise = []\n",
        "#   left_lower_arm_sequence_noise = []\n",
        "#   left_hand_sequence_noise = []\n",
        "\n",
        "#   right_upper_arm_sequence_noise = []\n",
        "#   right_lower_arm_sequence_noise = []\n",
        "#   right_hand_sequence_noise = []\n",
        "\n",
        "#   left_upper_leg_sequence_noise = []\n",
        "#   left_lower_leg_sequence_noise = []\n",
        "#   left_feet_sequence_noise = []\n",
        "\n",
        "#   right_upper_leg_sequence_noise = []\n",
        "#   right_lower_leg_sequence_noise = []\n",
        "#   right_feet_sequence_noise = []\n",
        "\n",
        "#   body_sequence_noise = []\n",
        "#   head_sequence_noise = []\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-HDJenhVQLx"
      },
      "source": [
        "reinforce your noise/incorrect dataset by having correct data but with randomized index replaced with random sequence\n",
        "\n",
        "in theory this would make the model more sensitive to changes\n",
        "\n",
        "we may also add values unto the model to simulate certain movement going more than what is intended"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv3pVdqfVQCC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR4sg22t8lxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a652c3-18a2-427e-bbbc-efe9868df4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------correct data augmentation ----------------------------\n",
            "Most Common Data Points: [17, 16, 19, 18, 20, 21, 22, 12, 15, 9, 14, 10, 6]\n",
            "Outlier Frequencies: [5, 26, 27, 37]\n",
            "-------------------applied frequency outlier detection-------------------\n",
            "original num ->  102\n",
            "current num ->  98\n",
            "removed num ->  4\n",
            "-------------------applied z-score outlier detection-------------------\n",
            "datapoints included ->  [19, 17, 16, 14, 18, 20, 12, 15]\n",
            "original num ->  98\n",
            "current num ->  82\n",
            "removed num ->  16\n",
            "22\n",
            "------------------------applied paddingV2------------------------\n",
            "max_length ->  20\n",
            "expanded_max_length ->  22\n",
            "original num set of sequences ->  82\n",
            "final num set of sequences ->  470\n",
            "470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "populate_0_input: 100%|██████████| 470/470 [00:00<00:00, 40409.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concat ->  470\n",
            "\n",
            "----------------------------data noise data augmentation ----------------------------\n",
            "22\n",
            "------------------------applied paddingV2------------------------\n",
            "max_length ->  0\n",
            "expanded_max_length ->  22\n",
            "original num set of sequences ->  160\n",
            "final num set of sequences ->  1550\n",
            "1550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "populate_0_input: 100%|██████████| 1550/1550 [00:00<00:00, 28736.36it/s]\n",
            "data_aug_seq_sensitivity: 100%|██████████| 470/470 [00:00<00:00, 1355.08it/s]\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aug_noise_data5---> (3570, 22, 66)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 52s 716ms/step - loss: 0.6945 - accuracy: 0.4894 - val_loss: 0.6922 - val_accuracy: 0.5248\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.6931 - accuracy: 0.4894 - val_loss: 0.6917 - val_accuracy: 0.5248\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.6923 - accuracy: 0.4894 - val_loss: 0.6911 - val_accuracy: 0.5248\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.6921 - accuracy: 0.4894 - val_loss: 0.6903 - val_accuracy: 0.5248\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 296ms/step - loss: 0.6913 - accuracy: 0.4894 - val_loss: 0.6890 - val_accuracy: 0.5248\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 2s 749ms/step - loss: 0.6901 - accuracy: 0.5137 - val_loss: 0.6870 - val_accuracy: 0.5248\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 2s 690ms/step - loss: 0.6889 - accuracy: 0.4924 - val_loss: 0.6834 - val_accuracy: 0.5248\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.6850 - accuracy: 0.5258 - val_loss: 0.6793 - val_accuracy: 0.5319\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 297ms/step - loss: 0.6796 - accuracy: 0.4924 - val_loss: 0.6693 - val_accuracy: 0.5248\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 303ms/step - loss: 0.6705 - accuracy: 0.4924 - val_loss: 0.6461 - val_accuracy: 0.5248\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.6566 - accuracy: 0.5106 - val_loss: 0.6516 - val_accuracy: 0.8191\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.6433 - accuracy: 0.5866 - val_loss: 0.6296 - val_accuracy: 0.8085\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.6330 - accuracy: 0.7447 - val_loss: 0.6086 - val_accuracy: 0.5248\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.6329 - accuracy: 0.5410 - val_loss: 0.6546 - val_accuracy: 0.7766\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.6482 - accuracy: 0.8131 - val_loss: 0.5637 - val_accuracy: 0.6809\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 388ms/step - loss: 0.6021 - accuracy: 0.5365 - val_loss: 0.5945 - val_accuracy: 0.8369\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.5978 - accuracy: 0.7903 - val_loss: 0.5341 - val_accuracy: 0.6277\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 296ms/step - loss: 0.5720 - accuracy: 0.5669 - val_loss: 0.5420 - val_accuracy: 0.8298\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.5499 - accuracy: 0.7219 - val_loss: 0.5286 - val_accuracy: 0.8333\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 2s 415ms/step - loss: 0.5421 - accuracy: 0.8313 - val_loss: 0.4777 - val_accuracy: 0.8227\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.5328 - accuracy: 0.8146 - val_loss: 0.4587 - val_accuracy: 0.8511\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.4866 - accuracy: 0.8161 - val_loss: 0.4537 - val_accuracy: 0.8794\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.5050 - accuracy: 0.8116 - val_loss: 0.5841 - val_accuracy: 0.6879\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.4854 - accuracy: 0.8055 - val_loss: 0.4420 - val_accuracy: 0.8475\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.4680 - accuracy: 0.8419 - val_loss: 0.4461 - val_accuracy: 0.8582\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.4338 - accuracy: 0.8647 - val_loss: 0.4530 - val_accuracy: 0.7979\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.3800 - accuracy: 0.8754 - val_loss: 0.3013 - val_accuracy: 0.9149\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.3873 - accuracy: 0.8450 - val_loss: 0.6658 - val_accuracy: 0.7801\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.6208 - accuracy: 0.7097 - val_loss: 0.7570 - val_accuracy: 0.4823\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.6717 - accuracy: 0.5486 - val_loss: 0.5925 - val_accuracy: 0.7553\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 2s 722ms/step - loss: 0.5674 - accuracy: 0.8419 - val_loss: 0.5801 - val_accuracy: 0.5674\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 2s 732ms/step - loss: 0.6029 - accuracy: 0.5380 - val_loss: 0.5922 - val_accuracy: 0.5284\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.5793 - accuracy: 0.6581 - val_loss: 0.5565 - val_accuracy: 0.8865\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.5696 - accuracy: 0.8845 - val_loss: 0.5562 - val_accuracy: 0.8830\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 337ms/step - loss: 0.5487 - accuracy: 0.8769 - val_loss: 0.5000 - val_accuracy: 0.8014\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 336ms/step - loss: 0.5285 - accuracy: 0.7325 - val_loss: 0.4649 - val_accuracy: 0.8582\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.4934 - accuracy: 0.8571 - val_loss: 0.4706 - val_accuracy: 0.8546\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.4679 - accuracy: 0.8647 - val_loss: 0.4127 - val_accuracy: 0.8688\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.4194 - accuracy: 0.8587 - val_loss: 0.3940 - val_accuracy: 0.8050\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.3744 - accuracy: 0.8663 - val_loss: 0.3613 - val_accuracy: 0.8404\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.3613 - accuracy: 0.8526 - val_loss: 0.3397 - val_accuracy: 0.8688\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 333ms/step - loss: 0.3859 - accuracy: 0.8587 - val_loss: 0.3146 - val_accuracy: 0.8794\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 396ms/step - loss: 0.3446 - accuracy: 0.8799 - val_loss: 0.3881 - val_accuracy: 0.8191\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 2s 719ms/step - loss: 0.3581 - accuracy: 0.8541 - val_loss: 0.4154 - val_accuracy: 0.8617\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 2s 663ms/step - loss: 0.3656 - accuracy: 0.8708 - val_loss: 0.4226 - val_accuracy: 0.7872\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 2s 627ms/step - loss: 0.3253 - accuracy: 0.8663 - val_loss: 0.4221 - val_accuracy: 0.8830\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 2s 819ms/step - loss: 0.3443 - accuracy: 0.8693 - val_loss: 0.4481 - val_accuracy: 0.7979\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 2s 743ms/step - loss: 0.3459 - accuracy: 0.8647 - val_loss: 0.2676 - val_accuracy: 0.8972\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 423ms/step - loss: 0.3778 - accuracy: 0.8374 - val_loss: 0.2674 - val_accuracy: 0.9007\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 0.3352 - accuracy: 0.8769 - val_loss: 0.2758 - val_accuracy: 0.8972\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.3133 - accuracy: 0.8784 - val_loss: 0.2835 - val_accuracy: 0.8936\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 0.2787 - accuracy: 0.9149 - val_loss: 0.2960 - val_accuracy: 0.9291\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 337ms/step - loss: 0.2992 - accuracy: 0.8936 - val_loss: 0.2834 - val_accuracy: 0.8901\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.2811 - accuracy: 0.9027 - val_loss: 0.2707 - val_accuracy: 0.9149\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 0.2713 - accuracy: 0.9012 - val_loss: 0.2632 - val_accuracy: 0.9078\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 2s 721ms/step - loss: 0.2423 - accuracy: 0.9301 - val_loss: 0.2572 - val_accuracy: 0.9184\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 2s 544ms/step - loss: 0.2327 - accuracy: 0.9149 - val_loss: 0.2703 - val_accuracy: 0.9043\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 329ms/step - loss: 0.2371 - accuracy: 0.9179 - val_loss: 0.2624 - val_accuracy: 0.9291\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.2301 - accuracy: 0.9301 - val_loss: 0.2687 - val_accuracy: 0.9113\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.2237 - accuracy: 0.9195 - val_loss: 0.2755 - val_accuracy: 0.9291\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.2427 - accuracy: 0.9179 - val_loss: 0.3178 - val_accuracy: 0.8901\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.2512 - accuracy: 0.9012 - val_loss: 0.2770 - val_accuracy: 0.9326\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.2345 - accuracy: 0.9255 - val_loss: 0.2490 - val_accuracy: 0.9149\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.2280 - accuracy: 0.9286 - val_loss: 0.2452 - val_accuracy: 0.9326\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.2134 - accuracy: 0.9422 - val_loss: 0.2447 - val_accuracy: 0.9149\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.2104 - accuracy: 0.9331 - val_loss: 0.2589 - val_accuracy: 0.9326\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.2099 - accuracy: 0.9301 - val_loss: 0.2658 - val_accuracy: 0.9184\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 438ms/step - loss: 0.2111 - accuracy: 0.9331 - val_loss: 0.2604 - val_accuracy: 0.9397\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 2s 644ms/step - loss: 0.2191 - accuracy: 0.9271 - val_loss: 0.2486 - val_accuracy: 0.9113\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 2s 489ms/step - loss: 0.2228 - accuracy: 0.9255 - val_loss: 0.2507 - val_accuracy: 0.9362\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 0.2290 - accuracy: 0.9286 - val_loss: 0.2319 - val_accuracy: 0.9255\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.2207 - accuracy: 0.9301 - val_loss: 0.2312 - val_accuracy: 0.9326\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.2062 - accuracy: 0.9362 - val_loss: 0.2318 - val_accuracy: 0.9184\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 0.2087 - accuracy: 0.9377 - val_loss: 0.2367 - val_accuracy: 0.9326\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.2000 - accuracy: 0.9301 - val_loss: 0.2372 - val_accuracy: 0.9326\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.2451 - accuracy: 0.9149 - val_loss: 0.2897 - val_accuracy: 0.8972\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.2371 - accuracy: 0.9164 - val_loss: 0.2664 - val_accuracy: 0.9326\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.2318 - accuracy: 0.9225 - val_loss: 0.2414 - val_accuracy: 0.9184\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 0.2197 - accuracy: 0.9195 - val_loss: 0.2460 - val_accuracy: 0.9397\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.2213 - accuracy: 0.9347 - val_loss: 0.2312 - val_accuracy: 0.9184\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 393ms/step - loss: 0.2052 - accuracy: 0.9347 - val_loss: 0.2297 - val_accuracy: 0.9397\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 2s 711ms/step - loss: 0.2144 - accuracy: 0.9286 - val_loss: 0.2269 - val_accuracy: 0.9220\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 2s 533ms/step - loss: 0.1993 - accuracy: 0.9331 - val_loss: 0.2303 - val_accuracy: 0.9397\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 0.1999 - accuracy: 0.9331 - val_loss: 0.2310 - val_accuracy: 0.9184\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.2153 - accuracy: 0.9316 - val_loss: 0.2235 - val_accuracy: 0.9255\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 0.1936 - accuracy: 0.9362 - val_loss: 0.2226 - val_accuracy: 0.9255\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.2042 - accuracy: 0.9422 - val_loss: 0.2229 - val_accuracy: 0.9291\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.1975 - accuracy: 0.9301 - val_loss: 0.2279 - val_accuracy: 0.9326\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.1951 - accuracy: 0.9392 - val_loss: 0.2291 - val_accuracy: 0.9184\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.2010 - accuracy: 0.9316 - val_loss: 0.2860 - val_accuracy: 0.9007\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.2358 - accuracy: 0.9179 - val_loss: 0.2943 - val_accuracy: 0.8865\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 297ms/step - loss: 0.2289 - accuracy: 0.9164 - val_loss: 0.2575 - val_accuracy: 0.9362\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.2192 - accuracy: 0.9316 - val_loss: 0.2506 - val_accuracy: 0.9184\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 553ms/step - loss: 0.2134 - accuracy: 0.9255 - val_loss: 0.2626 - val_accuracy: 0.9255\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 2s 659ms/step - loss: 0.2364 - accuracy: 0.9134 - val_loss: 0.2437 - val_accuracy: 0.9220\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 365ms/step - loss: 0.2255 - accuracy: 0.9119 - val_loss: 0.2201 - val_accuracy: 0.9397\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.1888 - accuracy: 0.9271 - val_loss: 0.2212 - val_accuracy: 0.9220\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 294ms/step - loss: 0.1982 - accuracy: 0.9286 - val_loss: 0.2133 - val_accuracy: 0.9397\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.1918 - accuracy: 0.9331 - val_loss: 0.2064 - val_accuracy: 0.9291\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 0.1965 - accuracy: 0.9316 - val_loss: 0.2130 - val_accuracy: 0.9255\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.1988 - accuracy: 0.9362 - val_loss: 0.2106 - val_accuracy: 0.9397\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.1777 - accuracy: 0.9377 - val_loss: 0.2088 - val_accuracy: 0.9291\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 303ms/step - loss: 0.1759 - accuracy: 0.9362 - val_loss: 0.2169 - val_accuracy: 0.9397\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.1784 - accuracy: 0.9407 - val_loss: 0.2214 - val_accuracy: 0.9220\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 0.1868 - accuracy: 0.9225 - val_loss: 0.2193 - val_accuracy: 0.9397\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.1946 - accuracy: 0.9331 - val_loss: 0.2010 - val_accuracy: 0.9291\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 2s 670ms/step - loss: 0.1789 - accuracy: 0.9377 - val_loss: 0.2022 - val_accuracy: 0.9291\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 2s 732ms/step - loss: 0.1805 - accuracy: 0.9316 - val_loss: 0.2361 - val_accuracy: 0.9397\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.1873 - accuracy: 0.9377 - val_loss: 0.2186 - val_accuracy: 0.9255\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 1s 303ms/step - loss: 0.1943 - accuracy: 0.9347 - val_loss: 0.2028 - val_accuracy: 0.9291\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.1936 - accuracy: 0.9255 - val_loss: 0.1972 - val_accuracy: 0.9397\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.1784 - accuracy: 0.9438 - val_loss: 0.1944 - val_accuracy: 0.9326\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 1s 287ms/step - loss: 0.1802 - accuracy: 0.9438 - val_loss: 0.2030 - val_accuracy: 0.9397\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.1769 - accuracy: 0.9347 - val_loss: 0.2026 - val_accuracy: 0.9291\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.1932 - accuracy: 0.9362 - val_loss: 0.1989 - val_accuracy: 0.9362\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 1s 303ms/step - loss: 0.1851 - accuracy: 0.9331 - val_loss: 0.1958 - val_accuracy: 0.9397\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.1794 - accuracy: 0.9422 - val_loss: 0.1968 - val_accuracy: 0.9291\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.1790 - accuracy: 0.9301 - val_loss: 0.2030 - val_accuracy: 0.9397\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 0.1711 - accuracy: 0.9362 - val_loss: 0.2069 - val_accuracy: 0.9291\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 2s 636ms/step - loss: 0.1953 - accuracy: 0.9362 - val_loss: 0.1972 - val_accuracy: 0.9397\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 2s 689ms/step - loss: 0.1739 - accuracy: 0.9377 - val_loss: 0.2012 - val_accuracy: 0.9397\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.1962 - accuracy: 0.9331 - val_loss: 0.2245 - val_accuracy: 0.9255\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.2022 - accuracy: 0.9179 - val_loss: 0.3201 - val_accuracy: 0.8794\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.2239 - accuracy: 0.9134 - val_loss: 0.2467 - val_accuracy: 0.9113\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.2044 - accuracy: 0.9225 - val_loss: 0.2618 - val_accuracy: 0.8936\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 1s 338ms/step - loss: 0.2183 - accuracy: 0.9119 - val_loss: 0.2254 - val_accuracy: 0.9184\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.1905 - accuracy: 0.9195 - val_loss: 0.2522 - val_accuracy: 0.9184\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 1s 337ms/step - loss: 0.2051 - accuracy: 0.9331 - val_loss: 0.2402 - val_accuracy: 0.9078\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 1s 302ms/step - loss: 0.2096 - accuracy: 0.9255 - val_loss: 0.2137 - val_accuracy: 0.9397\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1921 - accuracy: 0.9286 - val_loss: 0.1931 - val_accuracy: 0.9362\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.1748 - accuracy: 0.9331 - val_loss: 0.2067 - val_accuracy: 0.9433\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 1s 354ms/step - loss: 0.1723 - accuracy: 0.9438 - val_loss: 0.1936 - val_accuracy: 0.9291\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 2s 656ms/step - loss: 0.1692 - accuracy: 0.9438 - val_loss: 0.1956 - val_accuracy: 0.9397\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 2s 650ms/step - loss: 0.1620 - accuracy: 0.9407 - val_loss: 0.1924 - val_accuracy: 0.9326\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1721 - accuracy: 0.9362 - val_loss: 0.1977 - val_accuracy: 0.9397\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 1s 304ms/step - loss: 0.1718 - accuracy: 0.9362 - val_loss: 0.1906 - val_accuracy: 0.9291\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.1819 - accuracy: 0.9331 - val_loss: 0.2002 - val_accuracy: 0.9433\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.1766 - accuracy: 0.9347 - val_loss: 0.1852 - val_accuracy: 0.9326\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.1697 - accuracy: 0.9392 - val_loss: 0.1905 - val_accuracy: 0.9433\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 0.1636 - accuracy: 0.9377 - val_loss: 0.1850 - val_accuracy: 0.9397\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.1574 - accuracy: 0.9483 - val_loss: 0.1899 - val_accuracy: 0.9433\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.1621 - accuracy: 0.9392 - val_loss: 0.1924 - val_accuracy: 0.9362\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.1696 - accuracy: 0.9377 - val_loss: 0.1945 - val_accuracy: 0.9397\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.1849 - accuracy: 0.9286 - val_loss: 0.2182 - val_accuracy: 0.9255\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.2043 - accuracy: 0.9164 - val_loss: 0.2563 - val_accuracy: 0.9113\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 2s 692ms/step - loss: 0.2083 - accuracy: 0.9347 - val_loss: 0.2028 - val_accuracy: 0.9220\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 2s 700ms/step - loss: 0.1812 - accuracy: 0.9301 - val_loss: 0.2076 - val_accuracy: 0.9362\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 0.1662 - accuracy: 0.9331 - val_loss: 0.1987 - val_accuracy: 0.9291\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 1s 340ms/step - loss: 0.1827 - accuracy: 0.9301 - val_loss: 0.2217 - val_accuracy: 0.9362\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.1862 - accuracy: 0.9255 - val_loss: 0.1852 - val_accuracy: 0.9326\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.1610 - accuracy: 0.9377 - val_loss: 0.1846 - val_accuracy: 0.9468\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.1645 - accuracy: 0.9377 - val_loss: 0.1966 - val_accuracy: 0.9433\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.1742 - accuracy: 0.9286 - val_loss: 0.1873 - val_accuracy: 0.9326\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.1706 - accuracy: 0.9347 - val_loss: 0.2039 - val_accuracy: 0.9362\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.1735 - accuracy: 0.9377 - val_loss: 0.1870 - val_accuracy: 0.9362\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 1s 296ms/step - loss: 0.1716 - accuracy: 0.9392 - val_loss: 0.1955 - val_accuracy: 0.9468\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.1857 - accuracy: 0.9286 - val_loss: 0.2306 - val_accuracy: 0.9149\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 0.2074 - accuracy: 0.9073 - val_loss: 0.2261 - val_accuracy: 0.9397\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 2s 692ms/step - loss: 0.1814 - accuracy: 0.9362 - val_loss: 0.1888 - val_accuracy: 0.9362\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 2s 630ms/step - loss: 0.1639 - accuracy: 0.9362 - val_loss: 0.1910 - val_accuracy: 0.9433\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.1601 - accuracy: 0.9392 - val_loss: 0.1917 - val_accuracy: 0.9397\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.1543 - accuracy: 0.9422 - val_loss: 0.2253 - val_accuracy: 0.9468\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.1604 - accuracy: 0.9347 - val_loss: 0.1948 - val_accuracy: 0.9468\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.1547 - accuracy: 0.9468 - val_loss: 0.1868 - val_accuracy: 0.9397\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 1s 325ms/step - loss: 0.1632 - accuracy: 0.9377 - val_loss: 0.1938 - val_accuracy: 0.9326\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.1663 - accuracy: 0.9331 - val_loss: 0.1913 - val_accuracy: 0.9433\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1855 - accuracy: 0.9301 - val_loss: 0.2247 - val_accuracy: 0.9255\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.2037 - accuracy: 0.9195 - val_loss: 0.2029 - val_accuracy: 0.9433\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.2007 - accuracy: 0.9271 - val_loss: 0.2351 - val_accuracy: 0.9113\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.2317 - accuracy: 0.8997 - val_loss: 0.1902 - val_accuracy: 0.9397\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 1s 519ms/step - loss: 0.1910 - accuracy: 0.9331 - val_loss: 0.2011 - val_accuracy: 0.9397\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 2s 647ms/step - loss: 0.1798 - accuracy: 0.9331 - val_loss: 0.2151 - val_accuracy: 0.9149\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 2s 483ms/step - loss: 0.1656 - accuracy: 0.9347 - val_loss: 0.3001 - val_accuracy: 0.9326\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.1781 - accuracy: 0.9316 - val_loss: 0.2103 - val_accuracy: 0.9149\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 1s 292ms/step - loss: 0.1889 - accuracy: 0.9286 - val_loss: 0.2052 - val_accuracy: 0.9433\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 0.1781 - accuracy: 0.9301 - val_loss: 0.2120 - val_accuracy: 0.9433\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.1683 - accuracy: 0.9301 - val_loss: 0.1966 - val_accuracy: 0.9362\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.1618 - accuracy: 0.9362 - val_loss: 0.2110 - val_accuracy: 0.9397\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.1624 - accuracy: 0.9438 - val_loss: 0.1856 - val_accuracy: 0.9362\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.1594 - accuracy: 0.9362 - val_loss: 0.1858 - val_accuracy: 0.9397\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 0.1522 - accuracy: 0.9422 - val_loss: 0.1938 - val_accuracy: 0.9433\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 0.1661 - accuracy: 0.9362 - val_loss: 0.1896 - val_accuracy: 0.9362\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.1749 - accuracy: 0.9438 - val_loss: 0.1882 - val_accuracy: 0.9433\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 1s 424ms/step - loss: 0.1651 - accuracy: 0.9422 - val_loss: 0.1904 - val_accuracy: 0.9468\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 2s 695ms/step - loss: 0.1572 - accuracy: 0.9377 - val_loss: 0.1855 - val_accuracy: 0.9468\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 2s 647ms/step - loss: 0.1596 - accuracy: 0.9438 - val_loss: 0.1792 - val_accuracy: 0.9397\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.1494 - accuracy: 0.9422 - val_loss: 0.1767 - val_accuracy: 0.9433\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 1s 296ms/step - loss: 0.1572 - accuracy: 0.9392 - val_loss: 0.1852 - val_accuracy: 0.9468\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 1s 350ms/step - loss: 0.1509 - accuracy: 0.9407 - val_loss: 0.1796 - val_accuracy: 0.9468\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 1s 351ms/step - loss: 0.1720 - accuracy: 0.9362 - val_loss: 0.1784 - val_accuracy: 0.9362\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.1690 - accuracy: 0.9377 - val_loss: 0.1790 - val_accuracy: 0.9397\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.1656 - accuracy: 0.9392 - val_loss: 0.1852 - val_accuracy: 0.9433\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.1551 - accuracy: 0.9453 - val_loss: 0.2071 - val_accuracy: 0.9362\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.1681 - accuracy: 0.9331 - val_loss: 0.2214 - val_accuracy: 0.9362\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1686 - accuracy: 0.9331 - val_loss: 0.1808 - val_accuracy: 0.9326\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 1s 303ms/step - loss: 0.1549 - accuracy: 0.9438 - val_loss: 0.1832 - val_accuracy: 0.9433\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 2s 633ms/step - loss: 0.1581 - accuracy: 0.9377 - val_loss: 0.1883 - val_accuracy: 0.9433\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 2s 708ms/step - loss: 0.1565 - accuracy: 0.9392 - val_loss: 0.1927 - val_accuracy: 0.9468\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 0.1542 - accuracy: 0.9407 - val_loss: 0.1739 - val_accuracy: 0.9468\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.1566 - accuracy: 0.9438 - val_loss: 0.1790 - val_accuracy: 0.9433\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.1373 - accuracy: 0.9514 - val_loss: 0.1941 - val_accuracy: 0.9291\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.1271 - accuracy: 0.9635 - val_loss: 0.2125 - val_accuracy: 0.9468\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 325ms/step - loss: 0.1284 - accuracy: 0.9635 - val_loss: 0.3867 - val_accuracy: 0.8262\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 338ms/step - loss: 0.3187 - accuracy: 0.8587 - val_loss: 0.5684 - val_accuracy: 0.8121\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.2431 - accuracy: 0.9240 - val_loss: 0.2713 - val_accuracy: 0.8582\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.2688 - accuracy: 0.8708 - val_loss: 0.4328 - val_accuracy: 0.8227\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.2261 - accuracy: 0.9210 - val_loss: 0.1999 - val_accuracy: 0.9504\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.1546 - accuracy: 0.9635 - val_loss: 0.1895 - val_accuracy: 0.9468\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.1235 - accuracy: 0.9711 - val_loss: 0.1640 - val_accuracy: 0.9433\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 2s 588ms/step - loss: 0.1060 - accuracy: 0.9711 - val_loss: 0.1587 - val_accuracy: 0.9468\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 2s 649ms/step - loss: 0.1038 - accuracy: 0.9681 - val_loss: 0.2771 - val_accuracy: 0.9184\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 297ms/step - loss: 0.1209 - accuracy: 0.9681 - val_loss: 0.1721 - val_accuracy: 0.9504\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.1419 - accuracy: 0.9590 - val_loss: 0.2452 - val_accuracy: 0.9326\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 343ms/step - loss: 0.1059 - accuracy: 0.9605 - val_loss: 0.1658 - val_accuracy: 0.9574\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.1103 - accuracy: 0.9620 - val_loss: 0.1739 - val_accuracy: 0.9539\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.0942 - accuracy: 0.9681 - val_loss: 0.1837 - val_accuracy: 0.9504\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.0941 - accuracy: 0.9666 - val_loss: 0.1551 - val_accuracy: 0.9610\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 354ms/step - loss: 0.0777 - accuracy: 0.9726 - val_loss: 0.2033 - val_accuracy: 0.9433\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.0770 - accuracy: 0.9711 - val_loss: 0.1548 - val_accuracy: 0.9539\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0899 - accuracy: 0.9696 - val_loss: 0.1573 - val_accuracy: 0.9539\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.0790 - accuracy: 0.9742 - val_loss: 0.2013 - val_accuracy: 0.9468\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 416ms/step - loss: 0.0883 - accuracy: 0.9681 - val_loss: 0.1370 - val_accuracy: 0.9645\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 2s 718ms/step - loss: 0.0923 - accuracy: 0.9681 - val_loss: 0.2312 - val_accuracy: 0.9397\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 2s 424ms/step - loss: 0.0924 - accuracy: 0.9711 - val_loss: 0.1304 - val_accuracy: 0.9645\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.0727 - accuracy: 0.9772 - val_loss: 0.1497 - val_accuracy: 0.9539\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 351ms/step - loss: 0.0641 - accuracy: 0.9787 - val_loss: 0.1248 - val_accuracy: 0.9645\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 0.0716 - accuracy: 0.9787 - val_loss: 0.1472 - val_accuracy: 0.9539\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.0743 - accuracy: 0.9742 - val_loss: 0.1228 - val_accuracy: 0.9645\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 304ms/step - loss: 0.0864 - accuracy: 0.9757 - val_loss: 0.1717 - val_accuracy: 0.9504\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 329ms/step - loss: 0.0788 - accuracy: 0.9742 - val_loss: 0.1202 - val_accuracy: 0.9574\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.1358 - val_accuracy: 0.9574\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0587 - accuracy: 0.9787 - val_loss: 0.1176 - val_accuracy: 0.9574\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.0553 - accuracy: 0.9863 - val_loss: 0.1148 - val_accuracy: 0.9645\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.1447 - val_accuracy: 0.9504\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 2s 671ms/step - loss: 0.0618 - accuracy: 0.9772 - val_loss: 0.1179 - val_accuracy: 0.9610\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 2s 726ms/step - loss: 0.0485 - accuracy: 0.9848 - val_loss: 0.1372 - val_accuracy: 0.9574\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.0505 - accuracy: 0.9833 - val_loss: 0.1214 - val_accuracy: 0.9610\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0622 - accuracy: 0.9802 - val_loss: 0.2314 - val_accuracy: 0.9326\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1117 - accuracy: 0.9574 - val_loss: 0.1091 - val_accuracy: 0.9716\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 0.0666 - accuracy: 0.9757 - val_loss: 0.1306 - val_accuracy: 0.9574\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0509 - accuracy: 0.9818 - val_loss: 0.1104 - val_accuracy: 0.9610\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0597 - accuracy: 0.9848 - val_loss: 0.1381 - val_accuracy: 0.9539\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.0630 - accuracy: 0.9802 - val_loss: 0.1050 - val_accuracy: 0.9716\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 0.0673 - accuracy: 0.9772 - val_loss: 0.1642 - val_accuracy: 0.9468\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.0748 - accuracy: 0.9726 - val_loss: 0.1395 - val_accuracy: 0.9539\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.0865 - accuracy: 0.9711 - val_loss: 0.2680 - val_accuracy: 0.9149\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 342ms/step - loss: 0.0992 - accuracy: 0.9635 - val_loss: 0.1090 - val_accuracy: 0.9610\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 2s 671ms/step - loss: 0.0708 - accuracy: 0.9802 - val_loss: 0.1313 - val_accuracy: 0.9504\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 2s 718ms/step - loss: 0.0739 - accuracy: 0.9742 - val_loss: 0.1012 - val_accuracy: 0.9645\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 304ms/step - loss: 0.0633 - accuracy: 0.9772 - val_loss: 0.1138 - val_accuracy: 0.9610\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.0628 - accuracy: 0.9742 - val_loss: 0.0988 - val_accuracy: 0.9716\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.0592 - accuracy: 0.9802 - val_loss: 0.1172 - val_accuracy: 0.9610\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.0518 - accuracy: 0.9848 - val_loss: 0.0936 - val_accuracy: 0.9716\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 0.0427 - accuracy: 0.9863 - val_loss: 0.1665 - val_accuracy: 0.9433\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.0674 - accuracy: 0.9742 - val_loss: 0.1327 - val_accuracy: 0.9539\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 302ms/step - loss: 0.0844 - accuracy: 0.9726 - val_loss: 0.2174 - val_accuracy: 0.9291\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0709 - accuracy: 0.9742 - val_loss: 0.0902 - val_accuracy: 0.9716\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.0579 - accuracy: 0.9757 - val_loss: 0.1319 - val_accuracy: 0.9504\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.0540 - accuracy: 0.9818 - val_loss: 0.0896 - val_accuracy: 0.9716\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.0537 - accuracy: 0.9848 - val_loss: 0.1107 - val_accuracy: 0.9610\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 2s 742ms/step - loss: 0.0422 - accuracy: 0.9848 - val_loss: 0.0979 - val_accuracy: 0.9645\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 2s 630ms/step - loss: 0.0470 - accuracy: 0.9833 - val_loss: 0.0852 - val_accuracy: 0.9716\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0419 - accuracy: 0.9878 - val_loss: 0.0859 - val_accuracy: 0.9716\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.0369 - accuracy: 0.9848 - val_loss: 0.0916 - val_accuracy: 0.9645\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.0366 - accuracy: 0.9848 - val_loss: 0.0768 - val_accuracy: 0.9752\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.0441 - accuracy: 0.9894 - val_loss: 0.1084 - val_accuracy: 0.9574\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.0400 - accuracy: 0.9833 - val_loss: 0.0797 - val_accuracy: 0.9716\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0489 - accuracy: 0.9818 - val_loss: 0.0988 - val_accuracy: 0.9610\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 345ms/step - loss: 0.0418 - accuracy: 0.9833 - val_loss: 0.0787 - val_accuracy: 0.9716\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.0361 - accuracy: 0.9848 - val_loss: 0.0891 - val_accuracy: 0.9681\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.0344 - accuracy: 0.9878 - val_loss: 0.0824 - val_accuracy: 0.9716\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 304ms/step - loss: 0.0320 - accuracy: 0.9909 - val_loss: 0.0798 - val_accuracy: 0.9716\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 483ms/step - loss: 0.0324 - accuracy: 0.9894 - val_loss: 0.0775 - val_accuracy: 0.9716\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 2s 789ms/step - loss: 0.0281 - accuracy: 0.9894 - val_loss: 0.0803 - val_accuracy: 0.9645\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 2s 443ms/step - loss: 0.0343 - accuracy: 0.9909 - val_loss: 0.0789 - val_accuracy: 0.9645\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.0306 - accuracy: 0.9863 - val_loss: 0.0684 - val_accuracy: 0.9716\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.1582 - val_accuracy: 0.9468\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.0783 - accuracy: 0.9726 - val_loss: 0.0654 - val_accuracy: 0.9787\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.0612 - accuracy: 0.9802 - val_loss: 0.0701 - val_accuracy: 0.9716\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.0502 - accuracy: 0.9802 - val_loss: 0.0812 - val_accuracy: 0.9681\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.0453 - accuracy: 0.9833 - val_loss: 0.0595 - val_accuracy: 0.9787\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.0275 - accuracy: 0.9924 - val_loss: 0.0633 - val_accuracy: 0.9787\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0326 - accuracy: 0.9863 - val_loss: 0.0639 - val_accuracy: 0.9787\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 289ms/step - loss: 0.0324 - accuracy: 0.9878 - val_loss: 0.0672 - val_accuracy: 0.9752\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0267 - accuracy: 0.9909 - val_loss: 0.0620 - val_accuracy: 0.9752\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 539ms/step - loss: 0.0260 - accuracy: 0.9939 - val_loss: 0.0713 - val_accuracy: 0.9681\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 2s 671ms/step - loss: 0.0261 - accuracy: 0.9894 - val_loss: 0.0576 - val_accuracy: 0.9752\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 403ms/step - loss: 0.0276 - accuracy: 0.9894 - val_loss: 0.0875 - val_accuracy: 0.9681\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.0390 - accuracy: 0.9833 - val_loss: 0.0566 - val_accuracy: 0.9752\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.0480 - accuracy: 0.9863 - val_loss: 0.0899 - val_accuracy: 0.9716\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 302ms/step - loss: 0.0344 - accuracy: 0.9878 - val_loss: 0.0502 - val_accuracy: 0.9752\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.0289 - accuracy: 0.9894 - val_loss: 0.0541 - val_accuracy: 0.9752\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 1s 303ms/step - loss: 0.0192 - accuracy: 0.9924 - val_loss: 0.0535 - val_accuracy: 0.9752\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9894\n",
            "Training stopped as validation accuracy reached 0.9787 and validation loss reached 0.0475\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 0.0240 - accuracy: 0.9894 - val_loss: 0.0475 - val_accuracy: 0.9787\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.6172 - accuracy: 0.8511 - val_loss: 0.6560 - val_accuracy: 0.8511\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.5524 - accuracy: 0.8131 - val_loss: 0.2257 - val_accuracy: 0.9397\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 334ms/step - loss: 0.2890 - accuracy: 0.8967 - val_loss: 0.3640 - val_accuracy: 0.8511\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.3495 - accuracy: 0.8830 - val_loss: 0.3099 - val_accuracy: 0.9433\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 2s 672ms/step - loss: 0.3840 - accuracy: 0.8617 - val_loss: 0.2566 - val_accuracy: 0.9468\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 2s 707ms/step - loss: 0.2943 - accuracy: 0.9149 - val_loss: 0.3378 - val_accuracy: 0.8369\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.2841 - accuracy: 0.8815 - val_loss: 0.1995 - val_accuracy: 0.9539\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.2529 - accuracy: 0.9073 - val_loss: 0.1572 - val_accuracy: 0.9326\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.2239 - accuracy: 0.9119 - val_loss: 0.1370 - val_accuracy: 0.9610\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1936 - accuracy: 0.9271 - val_loss: 0.1103 - val_accuracy: 0.9752\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.1942 - accuracy: 0.9301 - val_loss: 0.1209 - val_accuracy: 0.9504\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.1696 - accuracy: 0.9392 - val_loss: 0.1066 - val_accuracy: 0.9610\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.1746 - accuracy: 0.9255 - val_loss: 0.1166 - val_accuracy: 0.9468\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.1505 - accuracy: 0.9468 - val_loss: 0.0983 - val_accuracy: 0.9787\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.1393 - accuracy: 0.9468 - val_loss: 0.1253 - val_accuracy: 0.9539\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 1s 304ms/step - loss: 0.1430 - accuracy: 0.9422 - val_loss: 0.0981 - val_accuracy: 0.9716\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.1436 - accuracy: 0.9529 - val_loss: 0.1069 - val_accuracy: 0.9645\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 2s 777ms/step - loss: 0.1464 - accuracy: 0.9377 - val_loss: 0.0962 - val_accuracy: 0.9716\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 2s 572ms/step - loss: 0.1449 - accuracy: 0.9453 - val_loss: 0.0973 - val_accuracy: 0.9716\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.1260 - accuracy: 0.9514 - val_loss: 0.1087 - val_accuracy: 0.9752\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.1362 - accuracy: 0.9438 - val_loss: 0.0863 - val_accuracy: 0.9716\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 325ms/step - loss: 0.1127 - accuracy: 0.9544 - val_loss: 0.0702 - val_accuracy: 0.9787\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.1000 - accuracy: 0.9605 - val_loss: 0.0681 - val_accuracy: 0.9752\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.0976 - accuracy: 0.9574 - val_loss: 0.0662 - val_accuracy: 0.9787\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.1007 - accuracy: 0.9559 - val_loss: 0.0621 - val_accuracy: 0.9787\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.0955 - accuracy: 0.9590 - val_loss: 0.0606 - val_accuracy: 0.9787\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.0900 - accuracy: 0.9650 - val_loss: 0.0622 - val_accuracy: 0.9787\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0835 - accuracy: 0.9666 - val_loss: 0.0618 - val_accuracy: 0.9787\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.0926 - accuracy: 0.9620 - val_loss: 0.0552 - val_accuracy: 0.9823\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 496ms/step - loss: 0.0814 - accuracy: 0.9681 - val_loss: 0.0560 - val_accuracy: 0.9823\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 2s 698ms/step - loss: 0.0789 - accuracy: 0.9681 - val_loss: 0.0507 - val_accuracy: 0.9823\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9696\n",
            "Training stopped as validation accuracy reached 0.9823 and validation loss reached 0.0490\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.0786 - accuracy: 0.9696 - val_loss: 0.0490 - val_accuracy: 0.9823\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9909\n",
            "Training stopped as validation accuracy reached 0.9894 and validation loss reached 0.0240\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 0.0332 - accuracy: 0.9909 - val_loss: 0.0240 - val_accuracy: 0.9894\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9818\n",
            "Training stopped as validation accuracy reached 0.9823 and validation loss reached 0.0229\n",
            "3/3 [==============================] - 1s 352ms/step - loss: 0.0612 - accuracy: 0.9818 - val_loss: 0.0229 - val_accuracy: 0.9823\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 349ms/step - loss: 0.1734 - accuracy: 0.9422 - val_loss: 0.2164 - val_accuracy: 0.9539\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 466ms/step - loss: 0.1849 - accuracy: 0.9559 - val_loss: 0.2264 - val_accuracy: 0.9113\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 2s 671ms/step - loss: 0.1410 - accuracy: 0.9529 - val_loss: 0.1565 - val_accuracy: 0.9645\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 2s 434ms/step - loss: 0.1337 - accuracy: 0.9574 - val_loss: 0.1274 - val_accuracy: 0.9645\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 297ms/step - loss: 0.1358 - accuracy: 0.9514 - val_loss: 0.1275 - val_accuracy: 0.9645\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.1053 - accuracy: 0.9650 - val_loss: 0.1221 - val_accuracy: 0.9645\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.1032 - accuracy: 0.9620 - val_loss: 0.1385 - val_accuracy: 0.9645\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.1035 - accuracy: 0.9635 - val_loss: 0.1371 - val_accuracy: 0.9539\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 1s 350ms/step - loss: 0.1087 - accuracy: 0.9574 - val_loss: 0.1300 - val_accuracy: 0.9645\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 1s 335ms/step - loss: 0.1003 - accuracy: 0.9574 - val_loss: 0.1500 - val_accuracy: 0.9468\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.1007 - accuracy: 0.9681 - val_loss: 0.1415 - val_accuracy: 0.9574\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1140 - accuracy: 0.9574 - val_loss: 0.1177 - val_accuracy: 0.9610\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.1082 - accuracy: 0.9620 - val_loss: 0.1257 - val_accuracy: 0.9610\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.1011 - accuracy: 0.9574 - val_loss: 0.1164 - val_accuracy: 0.9539\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 1s 585ms/step - loss: 0.0980 - accuracy: 0.9574 - val_loss: 0.1139 - val_accuracy: 0.9645\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 2s 625ms/step - loss: 0.0978 - accuracy: 0.9590 - val_loss: 0.1112 - val_accuracy: 0.9539\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.0942 - accuracy: 0.9650 - val_loss: 0.1016 - val_accuracy: 0.9681\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 1s 344ms/step - loss: 0.0856 - accuracy: 0.9666 - val_loss: 0.0986 - val_accuracy: 0.9645\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 1s 336ms/step - loss: 0.0811 - accuracy: 0.9681 - val_loss: 0.1028 - val_accuracy: 0.9645\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0991 - accuracy: 0.9590 - val_loss: 0.0915 - val_accuracy: 0.9681\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 0.0846 - accuracy: 0.9711 - val_loss: 0.0849 - val_accuracy: 0.9681\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.0815 - accuracy: 0.9620 - val_loss: 0.0829 - val_accuracy: 0.9681\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0784 - accuracy: 0.9681 - val_loss: 0.0809 - val_accuracy: 0.9681\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 1s 368ms/step - loss: 0.0780 - accuracy: 0.9681 - val_loss: 0.0815 - val_accuracy: 0.9716\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 0.0745 - accuracy: 0.9696 - val_loss: 0.0794 - val_accuracy: 0.9716\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.0828 - val_accuracy: 0.9787\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.0690 - accuracy: 0.9696 - val_loss: 0.0868 - val_accuracy: 0.9681\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 2s 759ms/step - loss: 0.0707 - accuracy: 0.9696 - val_loss: 0.0981 - val_accuracy: 0.9681\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 2s 686ms/step - loss: 0.0941 - accuracy: 0.9590 - val_loss: 0.0719 - val_accuracy: 0.9752\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 1s 336ms/step - loss: 0.0843 - accuracy: 0.9605 - val_loss: 0.0717 - val_accuracy: 0.9752\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.0673 - accuracy: 0.9696 - val_loss: 0.0774 - val_accuracy: 0.9787\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 1s 302ms/step - loss: 0.0670 - accuracy: 0.9650 - val_loss: 0.0693 - val_accuracy: 0.9787\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.0642 - accuracy: 0.9666 - val_loss: 0.0772 - val_accuracy: 0.9787\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.0712 - accuracy: 0.9681 - val_loss: 0.0747 - val_accuracy: 0.9681\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 0.0720 - accuracy: 0.9666 - val_loss: 0.0727 - val_accuracy: 0.9752\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.0556 - accuracy: 0.9742 - val_loss: 0.0685 - val_accuracy: 0.9787\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.0573 - accuracy: 0.9666 - val_loss: 0.0742 - val_accuracy: 0.9752\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 0.0653 - accuracy: 0.9696 - val_loss: 0.0820 - val_accuracy: 0.9645\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.0743 - accuracy: 0.9681 - val_loss: 0.0909 - val_accuracy: 0.9645\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.0659 - accuracy: 0.9696 - val_loss: 0.0632 - val_accuracy: 0.9858\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 2s 705ms/step - loss: 0.0647 - accuracy: 0.9681 - val_loss: 0.0657 - val_accuracy: 0.9787\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 2s 556ms/step - loss: 0.0689 - accuracy: 0.9681 - val_loss: 0.0823 - val_accuracy: 0.9681\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.0564 - accuracy: 0.9742 - val_loss: 0.0649 - val_accuracy: 0.9752\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.0704 - accuracy: 0.9666 - val_loss: 0.1211 - val_accuracy: 0.9539\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.1031 - accuracy: 0.9590 - val_loss: 0.0600 - val_accuracy: 0.9858\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 1s 329ms/step - loss: 0.0596 - accuracy: 0.9696 - val_loss: 0.0598 - val_accuracy: 0.9894\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.0771 - accuracy: 0.9666 - val_loss: 0.1337 - val_accuracy: 0.9397\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.0863 - accuracy: 0.9666 - val_loss: 0.1294 - val_accuracy: 0.9610\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.0843 - accuracy: 0.9666 - val_loss: 0.0995 - val_accuracy: 0.9610\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.0847 - accuracy: 0.9696 - val_loss: 0.0648 - val_accuracy: 0.9787\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.1190 - accuracy: 0.9529 - val_loss: 0.3011 - val_accuracy: 0.9113\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.2687 - accuracy: 0.9103 - val_loss: 0.1045 - val_accuracy: 0.9645\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 1s 560ms/step - loss: 0.2570 - accuracy: 0.9058 - val_loss: 0.2072 - val_accuracy: 0.9539\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 2s 576ms/step - loss: 0.2383 - accuracy: 0.9164 - val_loss: 0.1158 - val_accuracy: 0.9645\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 2s 525ms/step - loss: 0.1713 - accuracy: 0.9347 - val_loss: 0.0798 - val_accuracy: 0.9716\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.1074 - accuracy: 0.9666 - val_loss: 0.0798 - val_accuracy: 0.9681\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 0.0875 - accuracy: 0.9590 - val_loss: 0.0842 - val_accuracy: 0.9681\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.0734 - accuracy: 0.9696 - val_loss: 0.0937 - val_accuracy: 0.9645\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.0689 - accuracy: 0.9726 - val_loss: 0.0812 - val_accuracy: 0.9681\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.0713 - accuracy: 0.9696 - val_loss: 0.0973 - val_accuracy: 0.9645\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.0729 - accuracy: 0.9726 - val_loss: 0.0757 - val_accuracy: 0.9787\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.0749 - accuracy: 0.9590 - val_loss: 0.0763 - val_accuracy: 0.9752\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 1s 336ms/step - loss: 0.0618 - accuracy: 0.9711 - val_loss: 0.0789 - val_accuracy: 0.9752\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.0533 - accuracy: 0.9711 - val_loss: 0.0727 - val_accuracy: 0.9787\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 0.0617 - accuracy: 0.9681 - val_loss: 0.0861 - val_accuracy: 0.9787\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 2s 653ms/step - loss: 0.0614 - accuracy: 0.9742 - val_loss: 0.0699 - val_accuracy: 0.9787\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 2s 697ms/step - loss: 0.0505 - accuracy: 0.9742 - val_loss: 0.0642 - val_accuracy: 0.9858\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0487 - accuracy: 0.9711 - val_loss: 0.0634 - val_accuracy: 0.9858\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 0.0468 - accuracy: 0.9772 - val_loss: 0.0620 - val_accuracy: 0.9858\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 1s 294ms/step - loss: 0.0455 - accuracy: 0.9757 - val_loss: 0.0761 - val_accuracy: 0.9787\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.0509 - accuracy: 0.9802 - val_loss: 0.0613 - val_accuracy: 0.9894\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 1s 305ms/step - loss: 0.0457 - accuracy: 0.9742 - val_loss: 0.2153 - val_accuracy: 0.9645\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 1s 333ms/step - loss: 0.1784 - accuracy: 0.9559 - val_loss: 0.1793 - val_accuracy: 0.9255\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 1s 309ms/step - loss: 0.1224 - accuracy: 0.9514 - val_loss: 0.0811 - val_accuracy: 0.9645\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.0819 - accuracy: 0.9696 - val_loss: 0.0914 - val_accuracy: 0.9752\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0775 - accuracy: 0.9726 - val_loss: 0.0659 - val_accuracy: 0.9858\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.0545 - accuracy: 0.9757 - val_loss: 0.0631 - val_accuracy: 0.9787\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.0556 - accuracy: 0.9681 - val_loss: 0.0654 - val_accuracy: 0.9858\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 1s 547ms/step - loss: 0.0530 - accuracy: 0.9772 - val_loss: 0.0656 - val_accuracy: 0.9858\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 2s 768ms/step - loss: 0.0500 - accuracy: 0.9742 - val_loss: 0.0696 - val_accuracy: 0.9823\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.0442 - accuracy: 0.9787 - val_loss: 0.0637 - val_accuracy: 0.9823\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.0435 - accuracy: 0.9757 - val_loss: 0.0683 - val_accuracy: 0.9823\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 0.0421 - accuracy: 0.9787 - val_loss: 0.0554 - val_accuracy: 0.9894\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 1s 347ms/step - loss: 0.0433 - accuracy: 0.9726 - val_loss: 0.0584 - val_accuracy: 0.9894\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.0437 - accuracy: 0.9833 - val_loss: 0.0529 - val_accuracy: 0.9894\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 0.0393 - accuracy: 0.9818 - val_loss: 0.0570 - val_accuracy: 0.9823\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.0362 - accuracy: 0.9818 - val_loss: 0.0515 - val_accuracy: 0.9894\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 1s 337ms/step - loss: 0.0341 - accuracy: 0.9848 - val_loss: 0.0517 - val_accuracy: 0.9894\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0311 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9858\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0264 - accuracy: 0.9894 - val_loss: 0.0504 - val_accuracy: 0.9894\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.0279 - accuracy: 0.9848 - val_loss: 0.0512 - val_accuracy: 0.9894\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 2s 667ms/step - loss: 0.0468 - accuracy: 0.9787 - val_loss: 0.1591 - val_accuracy: 0.9468\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 2s 754ms/step - loss: 0.1048 - accuracy: 0.9696 - val_loss: 0.1718 - val_accuracy: 0.9362\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.0917 - accuracy: 0.9635 - val_loss: 0.0649 - val_accuracy: 0.9858\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.0814 - val_accuracy: 0.9610\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 0.0565 - accuracy: 0.9818 - val_loss: 0.0727 - val_accuracy: 0.9823\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.0531 - accuracy: 0.9787 - val_loss: 0.0609 - val_accuracy: 0.9858\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.0456 - accuracy: 0.9726 - val_loss: 0.0550 - val_accuracy: 0.9894\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 0.0369 - accuracy: 0.9802 - val_loss: 0.0575 - val_accuracy: 0.9894\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.0326 - accuracy: 0.9848 - val_loss: 0.0577 - val_accuracy: 0.9858\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.0347 - accuracy: 0.9787 - val_loss: 0.0567 - val_accuracy: 0.9894\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.0317 - accuracy: 0.9863 - val_loss: 0.0522 - val_accuracy: 0.9894\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.0310 - accuracy: 0.9848 - val_loss: 0.0648 - val_accuracy: 0.9823\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 1s 360ms/step - loss: 0.0521 - accuracy: 0.9787 - val_loss: 0.0580 - val_accuracy: 0.9894\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 2s 741ms/step - loss: 0.0407 - accuracy: 0.9802 - val_loss: 0.0606 - val_accuracy: 0.9858\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 2s 538ms/step - loss: 0.0299 - accuracy: 0.9863 - val_loss: 0.0539 - val_accuracy: 0.9858\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 1s 338ms/step - loss: 0.0292 - accuracy: 0.9894 - val_loss: 0.0538 - val_accuracy: 0.9894\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9909\n",
            "Training stopped as validation accuracy reached 0.9894 and validation loss reached 0.0479\n",
            "3/3 [==============================] - 1s 338ms/step - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.0479 - val_accuracy: 0.9894\n",
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 334ms/step - loss: 0.1689 - accuracy: 0.9574 - val_loss: 0.1769 - val_accuracy: 0.9362\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.1640 - accuracy: 0.9453 - val_loss: 0.0544 - val_accuracy: 0.9716\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 1s 316ms/step - loss: 0.1395 - accuracy: 0.9590 - val_loss: 0.0559 - val_accuracy: 0.9752\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.2765 - accuracy: 0.9149 - val_loss: 0.0638 - val_accuracy: 0.9752\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.1958 - accuracy: 0.9453 - val_loss: 0.0898 - val_accuracy: 0.9752\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 0.1521 - accuracy: 0.9407 - val_loss: 0.0925 - val_accuracy: 0.9574\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.1370 - accuracy: 0.9529 - val_loss: 0.1840 - val_accuracy: 0.9397\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 1s 356ms/step - loss: 0.1325 - accuracy: 0.9529 - val_loss: 0.0755 - val_accuracy: 0.9610\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 2s 614ms/step - loss: 0.1103 - accuracy: 0.9544 - val_loss: 0.0693 - val_accuracy: 0.9716\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 2s 652ms/step - loss: 0.0928 - accuracy: 0.9696 - val_loss: 0.0527 - val_accuracy: 0.9823\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9726\n",
            "Training stopped as validation accuracy reached 0.9823 and validation loss reached 0.0420\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.0890 - accuracy: 0.9726 - val_loss: 0.0420 - val_accuracy: 0.9823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 710ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Done. The result of TensorFlow matches the result of TensorFlow Lite.\n"
          ]
        }
      ],
      "source": [
        "# this is as a whole exercise\n",
        "def streamlined_process(correct_execution,noise_data):\n",
        "  id_num = str(rand.randint(1000,9999))\n",
        "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
        "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
        "\n",
        "  loop = 0\n",
        "\n",
        "  temp_correct_start=[]\n",
        "  temp_correct_wrong=[]\n",
        "\n",
        "  temp_wrong_store_train = []\n",
        "\n",
        "  data = base_data[0]\n",
        "  data_noise = base_data_noise[0]\n",
        "\n",
        "  best_val_loss = float('inf')\n",
        "  best_val_accuracy = 0.0\n",
        "  best_model = None\n",
        "\n",
        "\n",
        "\n",
        "  print('')\n",
        "  print('----------------------------correct data augmentation ----------------------------')\n",
        "  aug = common_length_sequence(data)\n",
        "  aug2 = apply_z_score(aug,1)\n",
        "  aug3 = paddingV2(aug2)\n",
        "  aug4 = populate_0_input(aug3,data_noise)\n",
        "  aug4 = np.array(aug4)\n",
        "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
        "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
        "  combined_inputs = aug4\n",
        "  print('concat -> ', len(combined_inputs))\n",
        "\n",
        "\n",
        "  print('')\n",
        "  print('----------------------------data noise data augmentation ----------------------------')\n",
        "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
        "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
        "\n",
        "# testing...\n",
        "  # data_aug_coor_sensitivity()\n",
        "\n",
        "# original\n",
        "  # aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
        "\n",
        "# testing(temporary)\n",
        "  aug_noise_data3 = aug_noise_data2[:]\n",
        "\n",
        "  aug_noise_data4 = np.array(aug_noise_data3)\n",
        "\n",
        "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
        "\n",
        "# testing(temporary)\n",
        "  aug_noise_data7 = data_aug_coor_sensitivity(aug_noise_data5,num_sequence_edit=int(len(aug_noise_data5[0])*.70))\n",
        "\n",
        "  # aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,2)\n",
        "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,int(len(combined_inputs)*.35))\n",
        "\n",
        "# original\n",
        "  # aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
        "\n",
        "# testing(temporary)\n",
        "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5,aug_noise_data7), axis = 0)\n",
        "\n",
        "\n",
        "  print('aug_noise_data5--->',aug_noise_data5.shape)\n",
        "\n",
        "\n",
        "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(aug4[0]), len(aug4[0][0]))))\n",
        "  model.add(LSTM(len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu'))\n",
        "  model.add(Bidirectional(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True, dropout=0.3, recurrent_dropout=0.3, activation='relu')))\n",
        "  model.add(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False, activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Dense(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .3)), activation='relu'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
        "  model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for x in range(loop):\n",
        "    temp_wrong_store_train = aug_noise_data5[x*len(combined_inputs):len(combined_inputs)*(x+1)]\n",
        "\n",
        "    aug3_label = np.ones(len(combined_inputs))\n",
        "    aug_noise_label = np.zeros(len(temp_wrong_store_train))\n",
        "\n",
        "    rand_batches=concatenate_randomize_batches(combined_inputs,aug3_label,temp_wrong_store_train,aug_noise_label)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.3, random_state=42)\n",
        "\n",
        "    # history=model.fit(X_train, y_train, epochs=200, batch_size =256  , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
        "    history=model.fit(X_train, y_train, epochs=200, batch_size =256  , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    val_accuracy = max(history.history['val_accuracy'])\n",
        "    temp_wrong_store_train=[]\n",
        "\n",
        "\n",
        "    if val_loss < best_val_loss and val_accuracy > best_val_accuracy:\n",
        "      best_model = model.get_weights()\n",
        "      best_val_loss = val_loss\n",
        "      best_val_accuracy = val_accuracy\n",
        "\n",
        "    # testing(temporary)\n",
        "    model.set_weights(best_model)\n",
        "\n",
        "  X_train = X_train.astype(np.float32)\n",
        "  X_test = X_test.astype(np.float32)\n",
        "  model.save('testingModel')\n",
        "  convert_tf_to_tflite('/content/testingModel',[1,len(aug3[0]),len(aug3[0][0])], X_test,'whole_model',id_num,val_loss,val_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# uncomment this to run training model as a whole(this means that this model can determine the motion as a whole but not specify what part if correct or not)\n",
        "streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUdK6q-YbfwM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_fhhgZGOTMS",
        "outputId": "1cedc8a2-dd44-4748-a348-863ac6e56a89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# this is for the individual models\n",
        "# this is experimental\n",
        "def streamlined_process(correct_execution,noise_data,loop=5):\n",
        "  id_num = str(rand.randint(1000,9999))\n",
        "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
        "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
        "\n",
        "  temp_correct_start=[]\n",
        "  temp_correct_wrong=[]\n",
        "\n",
        "  data = base_data[0]\n",
        "  data_noise = base_data_noise[0]\n",
        "\n",
        "\n",
        "\n",
        "  print('')\n",
        "  print('----------------------------correct data augmentation ----------------------------')\n",
        "  aug = common_length_sequence(data)\n",
        "  aug2 = apply_z_score(aug,1)\n",
        "  aug3 = paddingV2(aug2)\n",
        "  aug4 = populate_0_input(aug3,data_noise)\n",
        "  aug4 = np.array(aug4)\n",
        "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
        "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
        "  combined_inputs= aug4\n",
        "  print('concat -> ', len(combined_inputs))\n",
        "\n",
        "\n",
        "\n",
        "  print('')\n",
        "  print('----------------------------data noise data augmentation ----------------------------')\n",
        "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
        "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
        "  aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
        "  aug_noise_data4 = np.array(aug_noise_data3)\n",
        "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
        "\n",
        "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,1,int(len(combined_inputs)*.35))\n",
        "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
        "  # aug_noise_data6 = data_aug_coor_sensitivity(aug4)\n",
        "  # aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
        "\n",
        "\n",
        "  correct_data_set = []\n",
        "  noise_data_set = []\n",
        "  rand_batches = []\n",
        "\n",
        "  print('testtesttesttest=======>',len(aug_noise_data5))\n",
        "\n",
        "\n",
        "  for x in range(14):\n",
        "    correct_data_set.append([])\n",
        "    noise_data_set.append([])\n",
        "    rand_batches.append([])\n",
        "\n",
        "\n",
        "  for exercise in combined_inputs:\n",
        "\n",
        "    left_upper_arm = []\n",
        "    left_lower_arm = []\n",
        "    left_hand = []\n",
        "    right_upper_arm = []\n",
        "    right_lower_arm = []\n",
        "    right_hand = []\n",
        "    left_upper_leg = []\n",
        "    left_lower_leg = []\n",
        "    left_feet = []\n",
        "    right_upper_leg = []\n",
        "    right_lower_leg = []\n",
        "    right_feet = []\n",
        "    head =[]\n",
        "    body = []\n",
        "\n",
        "\n",
        "\n",
        "#------------------------ generating the correct input of certain part------------------------------\n",
        "    for sequence in exercise:\n",
        "      # 11,13\n",
        "      left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
        "      # 13,15\n",
        "      left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
        "      # 15,17,19,21\n",
        "      left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
        "\n",
        "      # 12,14\n",
        "      right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
        "      # 14,16\n",
        "      right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
        "      # 16,18,20,22\n",
        "      right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
        "\n",
        "      # 23,25\n",
        "      left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
        "      # 25,27\n",
        "      left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
        "      # 27,29,31\n",
        "      left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
        "\n",
        "      # 24,26\n",
        "      right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
        "      # 26,28\n",
        "      right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
        "      # 28,30,32\n",
        "      right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
        "\n",
        "      # 11,12,23,24\n",
        "      body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
        "      # 7,8,9,10\n",
        "      head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
        "\n",
        "\n",
        "    correct_data_set[0].append(left_upper_arm)\n",
        "    correct_data_set[1].append(left_lower_arm)\n",
        "    correct_data_set[2].append(left_hand)\n",
        "\n",
        "    correct_data_set[3].append(right_upper_arm)\n",
        "    correct_data_set[4].append(right_lower_arm)\n",
        "    correct_data_set[5].append(right_hand)\n",
        "\n",
        "    correct_data_set[6].append(left_upper_leg)\n",
        "    correct_data_set[7].append(left_lower_leg)\n",
        "    correct_data_set[8].append(left_feet)\n",
        "\n",
        "    correct_data_set[9].append(right_upper_leg)\n",
        "    correct_data_set[10].append(right_lower_leg)\n",
        "    correct_data_set[11].append(right_feet)\n",
        "\n",
        "    correct_data_set[12].append(body)\n",
        "    correct_data_set[13].append(head)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for x in range(len(correct_data_set)):\n",
        "    correct_data_set[x] = np.array(correct_data_set[x])\n",
        "\n",
        "  # for x in range(len(noise_data_set)):\n",
        "  #   noise_data_set[x] = np.array(noise_data_set[x])\n",
        "\n",
        "  print('len of correct data set ---->',len(correct_data_set[0]))\n",
        "  print('len of noise data set ---->',len(noise_data_set[0]))\n",
        "\n",
        "  correct_data_set_label = np.ones(len(combined_inputs))\n",
        "  noise_data_set_label = np.zeros(len(aug_noise_data5))\n",
        "\n",
        "  # for x in range(len(correct_data_set)):\n",
        "  #   rand_batches[x]=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set[x],noise_data_set_label)\n",
        "\n",
        "\n",
        "\n",
        "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
        "\n",
        "\n",
        "  data_set_name=['left_upper_arm','left_lower_arm','left_hand','right_upper_arm','right_lower_arm','right_hand','left_upper_leg','left_lower_leg','left_feet','right_upper_leg','right_lower_leg','right_feet','head','body']\n",
        "  for x in range(len(data_set_name)-1):\n",
        "    print('progress -> ',x,'/',len(data_set_name)-1)\n",
        "\n",
        "\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_accuracy = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    for y in range(loop):\n",
        "\n",
        "      noise_data_set=[]\n",
        "\n",
        "\n",
        "    #------------------------------ generating noise ---------------------------------------\n",
        "      for exercise in aug_noise_data5[len(correct_data_set[x])*y:len(correct_data_set[x]) + (len(correct_data_set[x])*y)]:\n",
        "      # for exercise in aug_noise_data5[0:450]:\n",
        "        # print('batchin noise ->',len(correct_data_set[x])*y,'<==--==>',len(correct_data_set[x]) + (len(correct_data_set[x])*y))\n",
        "\n",
        "\n",
        "        left_upper_arm = []\n",
        "        left_lower_arm = []\n",
        "        left_hand = []\n",
        "        right_upper_arm = []\n",
        "        right_lower_arm = []\n",
        "        right_hand = []\n",
        "        left_upper_leg = []\n",
        "        left_lower_leg = []\n",
        "        left_feet = []\n",
        "        right_upper_leg = []\n",
        "        right_lower_leg = []\n",
        "        right_feet = []\n",
        "        head =[]\n",
        "        body = []\n",
        "\n",
        "\n",
        "\n",
        "        for sequence in exercise:\n",
        "          # 11,13\n",
        "          # print(sequence[22])\n",
        "          if x == 0:\n",
        "            left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
        "          # 13,15\n",
        "          if x == 1:\n",
        "            left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
        "          # 15,17,19,21\n",
        "          if x == 2:\n",
        "            left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
        "\n",
        "          # 12,14\n",
        "          if x == 3:\n",
        "            right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
        "          # 14,16\n",
        "          if x == 4:\n",
        "            right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
        "          # 16,18,20,22\n",
        "          if x == 5:\n",
        "            right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
        "\n",
        "          # 23,25\n",
        "          if x == 6:\n",
        "            left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
        "          # 25,27\n",
        "          if x == 7:\n",
        "            left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
        "          # 27,29,31\n",
        "          if x == 8:\n",
        "            left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
        "\n",
        "          # 24,26\n",
        "          if x == 9:\n",
        "            right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
        "          # 26,28\n",
        "          if x == 10:\n",
        "            right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
        "          # 28,30,32\n",
        "          if x == 11:\n",
        "            right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
        "\n",
        "          # 11,12,23,24\n",
        "          if x == 12:\n",
        "            body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
        "          # 7,8,9,10\n",
        "          if x == 13:\n",
        "            head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
        "\n",
        "        if x == 0:\n",
        "          noise_data_set.append(left_upper_arm)\n",
        "        if x == 1:\n",
        "          noise_data_set.append(left_lower_arm)\n",
        "        if x == 2:\n",
        "          noise_data_set.append(left_hand)\n",
        "\n",
        "        if x == 3:\n",
        "          noise_data_set.append(right_upper_arm)\n",
        "        if x == 4:\n",
        "          noise_data_set.append(right_lower_arm)\n",
        "        if x == 5:\n",
        "          noise_data_set.append(right_hand)\n",
        "\n",
        "        if x == 6:\n",
        "          noise_data_set.append(left_upper_leg)\n",
        "        if x == 7:\n",
        "          noise_data_set.append(left_lower_leg)\n",
        "        if x == 8:\n",
        "          noise_data_set.append(left_feet)\n",
        "\n",
        "\n",
        "        if x == 9:\n",
        "          noise_data_set.append(right_upper_leg)\n",
        "        if x == 10:\n",
        "          noise_data_set.append(right_lower_leg)\n",
        "        if x == 11:\n",
        "          noise_data_set.append(right_feet)\n",
        "\n",
        "\n",
        "        if x == 12:\n",
        "          noise_data_set.append(body)\n",
        "        if x == 13:\n",
        "          noise_data_set.append(head)\n",
        "\n",
        "# ===================================================================================================================================================\n",
        "\n",
        "      print('correct->',len(correct_data_set[x]),'  incorrect->',len(noise_data_set[x]))\n",
        "\n",
        "      rand_batches=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set,noise_data_set_label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      print('loop -> ',y,'/',loop)\n",
        "      X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.2, random_state=42)\n",
        "\n",
        "      # history = model.fit(X_train[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_train[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)], epochs=200, batch_size =128 , validation_data=(X_test[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_test[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)]), callbacks=[custom_early_stopping])\n",
        "\n",
        "      if y == 0:\n",
        "        model_base_modifier = 10\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(model_base_modifier+len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(correct_data_set[x][0]), len(correct_data_set[x][0][0]))))\n",
        "        model.add(LSTM(model_base_modifier+len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu' ))\n",
        "        model.add(Bidirectional(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .5)), return_sequences=True, dropout=0.4, recurrent_dropout=0.4, activation='relu')))\n",
        "        model.add(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False,  activation='relu'))\n",
        "        # model.add(BatchNormalization())\n",
        "        model.add(Dense(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), activation='relu'))\n",
        "\n",
        "        model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "        custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
        "        model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "      history = model.fit(X_train, y_train, epochs=150, batch_size =128 , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
        "\n",
        "      val_loss = min(history.history['val_loss'])\n",
        "      val_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "      if val_loss < best_val_loss and val_accuracy > best_val_accuracy:\n",
        "        best_model = model.get_weights()\n",
        "        best_val_loss = val_loss\n",
        "        best_val_accuracy = val_accuracy\n",
        "\n",
        "      model.set_weights(best_model)\n",
        "\n",
        "\n",
        "\n",
        "    model.save('testingModel')\n",
        "\n",
        "\n",
        "    # X_train = X_train[x].astype(np.float32)\n",
        "    # X_test = X_test[x].astype(np.float32)\n",
        "\n",
        "    # convert_tf_to_tflite('/content/testingModel',[1,len(X_train[0]),len(X_train[0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
        "    # convert_tf_to_tflite('/content/testingModel',[1,len(correct_data_set[x][0]),len(correct_data_set[x][0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
        "\n",
        "\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "    model.save('testingModel')\n",
        "\n",
        "    print('param 1 ->',len(noise_data_set[0]) )\n",
        "    print('param 1 ->',len(noise_data_set[0][0]))\n",
        "    print('current x ->',x)\n",
        "    convert_tf_to_tflite('/content/testingModel',[1,len(noise_data_set[0]),len(noise_data_set[0][0])], X_test,data_set_name[x],id_num,best_val_loss,best_val_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# uncomment this to run the individual landmarks training training(this means that it can determine individual parts of the body if spefic parts are incorrect or correct)\n",
        "# streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n",
        "3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "analysis(sept 9 2023):\n",
        "\n",
        "paddingV2 function possibly causing problems\n",
        "\n",
        "made a function that augments individual coordinates can make or break, still needds more testing"
      ],
      "metadata": {
        "id": "kzxx7bTIw0-3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2z1s1sXxrQ5"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GFxKEsqxy8t",
        "outputId": "ecb48f49-6710-4d69-bcfc-9e028e0785db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}